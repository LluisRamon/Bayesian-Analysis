Chapter 2: Bayesian Inference
========================================================

--- Start Class on 4-3-2014 (session 7)

# 1.Posterior distribution as an estimator. 

Once you have computed (simulated) from $\pi(\theta| y)$, $p(\tilde y | y)$.

You are basically done.

The posterior distribution is a sufficient statistic. If the model is correct, you can throw away your data and stick only with $\pi(\theta | y)$.

One can claim that the Bayesian estimate of $\theta$ is $\pi(\theta | y)$.
One can claim that the Bayesian estimate of $\tilde y$ is $p_{\pi}(\tilde y | y)$.

Why does any Bayesian need anything else (point estimation, interval estimator, test, predictive intervals)?

* Compare with what frequentist do (cynic answer)
* Bayesian need to summarize what is in the posterior in order to communicate results.

Imagine that your parameter space $\theta = (\theta_1, \dots \theta_p) \in R^p$ and you only care about $\theta_1$. What do you do if you only have:

$\ell_{Y = y}(\theta_1, \dots \theta_p)$

$\tilde \ell(\theta_1) = max \ell_{Y = y}(\theta_1, \dots \theta_p)$

Why is this a good answer?

Bayesian answer will be the marginal:

$\pi(\theta_1 | y) = \int \dots \int \pi(\theta_1 \dots \theta_p |y) d\theta_1 \dots d\theta_p$

Imagine that you have worked things out for $\theta$ and now you ask questions about $g(\theta)$.

![alt text][Missing plot]

[Missing plot]: figure/missing-plot.png "MissingPlot"

Frequentist have difficulties translating inferences about $\theta$ into inferences about $g(\theta)$.

Bayesian will do $\pi(\theta | y) \rightarrow \pi(g(\theta) | y)$ just a change of variables.

$\theta_1 \dots \theta_p \rightarrow g(\theta_1) \dots g(\theta_p)$ sample from $\pi(g(\theta) | y)$.

# 2.Point estimation. 

![alt text][beta_prior_pos_2_2]
[beta_prior_pos_2_2]: figure/beta_prior_pos_2_2.png

How do we summarize the $formula$ with a number if we must?

* Expectation $formula$
* Mode $formula$
* Median $formula$
* etc

Observation:

* a) If $\pi(\theta)$ is flat then $\hat \theta_{pmode} = \hat \theta_{MLikelihood}$
* b) to compute the $\hat \theta_{pmode}$ you don't need to know the posterior distribution. $formula$ maximizing this is equivalent to maximizing $\pi(\theta | y)$.
* c) Under regularity conditions often satisfied $\hat \theta_{pmode}$ has the same asymptotic properties as $\hat \theta_{ML}$

You can only use the Median if you deal with real valued $\theta$. If $\theta \in \Omega R^2$ you can't sort $\Omega$.

What happens if your posterior is ??

![alt text][Missing plot]

What do you give one point estimation?
Do you want to give a point estimate? Probably not.

If you are in $R^10$ you probably want to do point estimate.

An estimator is neither Bayesian nor Frequentist.
An estimator is a function of the data $formula$ that hopefully will be close to the truth $\theta^*$ most of the time.

What will be Bayesian or Frequentist is how you judge (assess) the estimator.

1. Frequentsit assessment: $E_{y | \theta^*} ( | \theta) formula$
2. Bayesian assessment: $_{\theta | y} formula$

# 3.Interval (region) estimation. 

We define a region with posterior credibility p to be a subset of $\Omega$, $formula$, such that $\int_{C_{p}(y)}formula$

http://en.wikipedia.org/wiki/Credible_interval

![alt text][Missing plot]

The same concept applies to $formula$.

They are useful as summaries of the uncertainty in $formula$, $formula$, $formula$, $formula$.

There are 2 families of credibility regions

* a) HPD regions (Highest Posterior Density) They are the smallest regions that have credibility p.

![alt text][Missing plot]

You restrict yourself to picking the values of $\theta$ with the highest density.

  * They are the smallest. (strength)
  * They generalize well when $\theta \in R^p, p > 1$. (strength)
  * They are not easy to compute. Even if you have $\pi(\theta|y)$ in closed form it is not easy. (weakness)
  * They are not parametric invariant. (weakness)

![alt text][Missing plot]

They might no be connected.(fact)

* b) Central p-credible intervals

![alt text][Missing plot]

--- Start Class on 6-3-2014 (session 8)

  * You can not use the if you are not in the real line (weakness)
  * You are not getting the smallest p-credible region possible (weakness)
  * Easy to compute (strength)
  * It is easy to estimate from a sample simulated from $\pi(\theta), formula$ (strength)

$formula$ Sample simulated from $\pi(\theta | y)$ ordered from small to big

$\hat q^{\frac{1-p}{2}} = formula$

  * Invariant when you re-parametrize. (strength)
  
  ![alt text][Missing plot]
  
  * They have to be intervals (fact)


An interval $| a(y), b(y)|$ is neither Bayesian nor frequentist. What is Bayesian or Frequentist is the way in which you asses (judge) it.

A Bayesian will judge the interval through the posterior $\pi(\theta | y)$

$P_{\theta | y}(\theta \in | a(y), b(y)| | y) = p$ credible $\theta$ unknown $y$ fixed

A Frequentist will judge it through repeated sampling from $M = {P(y | \theta^*), \theta^* \in \Omega}$

$\inf_{\theta^* \in \Omega}  = P_{y|\theta^*}(\theta^* \in | a(y), b(y)| | \theta^*) = p(\theta^*)$ confidence of the interval.

$\theta$ fixed, $y$ random.

It is extreamly rare that $p$ and $1 - \alpha$ for a given $| a(y), b(y)|$.

There is a temptation of selling an 95% confidence interval as if it was 95% credible interval. This is cheating.

Confidence it is not a probability.

# 4.Two-hypothesis test. 

$\Omega = \Omega_1 \cup \Omega_2$ 

$M = {P(y | \theta), \theta \in \Omega} = formula$

$\left\{\begin{matrix}H_{1}: \theta \in \Omega_{1}\\H_{2}: \theta \in \Omega_{2}\end{matrix}\right.$

$M_1: \tilde y \sim M_1$
$M_2: \tilde y \sim M_2$

![alt text][Missing plot]

$formula$

You will choose the $H_1$ that has the largest posterior probability.

$formula$ posterior odds.

Note that we are treating the null and the alternative symetrically. Compute both probability and choose the one that has more probability.

There is no difference between $H_0$ and $H_a$.

$\underline{Example 1}$

Simple against simple. 

$M = {P(y | \theta), \theta \in {\theta_1, \theta_2}} = {p(y| \theta_1), p(y| \theta_2)}$ is a Dichothomy. 

$complex formula$

$P(H_1 | y) = \frac{formula}{formula}$
$P(H_2 | y) = \frac{formula}{formula}$

Posterior odds $\frac{P(H_1 | y)}{P(H_2 | y)} = \frac{P(H_1 | y)}{1 - P(H_1 | y)} = \frac{P(H_1)}{P(H_2)}\frac{P(y | \theta_1)}{P(y | \theta_2)} = \frac{P(H_1)}{P(H_2)}\frac{\ell_y(\theta_1)}{\ell_y(\theta_2)}$

Posterior odds = prior odds x likelihood ratio (Bayes factor)

Neyman-Pearson states that it is optimal to have a rejection region based  on 

$\frac{\ell_y(\theta_1)}{\ell_y(\theta_2)} = \frac{P(y | \theta_1)}{P(y | \theta_2)}$

C os a constant that depends on the size of your test.
If it is > C then $H_1$
If it is < C then $H_2$

$p-value = formula$

Only works for simple against simple.

Often we do as if the $p-value = P(H_1|y) = P(H_0|y)$.

Instances when a p-value is approximately equal to $P(H_1|y)$ are rare.


$\underline{Example 2}$

Chance between two submodels

$M = M_1 \cup M_2 = formula \cup formula$

${Poisson(\lambda), \lambda \in (0, \infty)} \cup {NegativeBinomial(r, \theta), \theta \in (0,1)}$

$\left\{\begin{matrix}H_{1}: \theta \in \Omega_{1}\\H_{2}: \theta \in \Omega_{2}\end{matrix}\right.$

$M_1: \tilde y \sim P_1(y | \theta)$ $P(H_1), \pi(\theta | H_1)$

$M_2: \tilde y \sim P_2(y | \theta)$ $P(H_1), \pi(\theta | H_2)$

$P(H_1 |y)$

Complex problems will be dealed as the simple case.

--- Start Class on 11-3-2014 (session 9)

$formulas$

Model selection = Hypothesis testing

$P(M_1 | y) = \frac{formula}{formula} = \frac{formula}{formula}$

$P(M_2 | y) = 1 - P(M_1 | y) = \frac{formula}{formula}$

Compute the odds ratio

$\frac{formula}{formula} = \frac{formula}{formula}$

$formula$ Prior odds
$formula$ Posterior odds
$formula$ Bayes factor

Both pices depends on the prior.

The [Bayes Factor][] can be written as $formula$

[Bayes Factor]: http://en.wikipedia.org/wiki/Bayes_factor

$formula$ Integrated likelihood under $M_1$ or marginal likelihood.
$formula$ Integrated likelihood under $M_2$ or marginal likelihood.

Likelihood ratio test: Chooses hypothesis (submodel) through:

Instead of judging $M_i$ based on the maximum of the likelihood we do it based on average of the likelihood.


$\lambda (x) = \frac{max P_{1}(y | \theta_1) formula}{max P_{2}(y | \theta_2)}$

The crucial thing to use when comparing models is the integrated likelihood, $P_{\pi_i}(y) = P(y | M_i)$.

You will choose depending on the ratio $\frac{formula}{formula} = \frac{formula}{formula}$ is bigger or smaller than 1. (???).

Bad news: Bayes factors make sense only under proper priors $\pi_1$ and $\pi_2$.

$\pi(\theta | y) = \frac{\pi(\theta)P(y|\theta)}{formula} =$

Here choosing an inproper prior is ok because the constant in $\pi(\theta)$, we cancel them.

$BF = formula$

Inproper priors are not welcome, because constant is not canceled.

$H_{1}: \theta  = \theta_{0}$
$H_{2}: \theta \neq \theta_{0}$

$P(H_{1}) = 1/2$
$P(H_{2}) = 1/2$

(dibuix with very long tails)

Bayesian estatitics is easy as you are confortable with your prior.

Note: If $H_1$ is true then

$P(H_1 | y) \rigtharrow 1 as n \rigtharrow \infty$
$P(H_2 | y) \rigtharrow 0 as n \rigtharrow \infty$

$M = M_1 \cup M_2$

(dibuix)

Choosing between submodel 1 or submodel 2

$M_1 \tilde y \sim P_1(\tilde y | \theta_1)$
$M_1 \tilde y \sim P_1(\tilde y | \theta_1)$

is the same as choosing simple against simple.

$H_1: \tilde y \sim P_{\pi_1}(\tilde y)$
$H_2: \tilde y \sim P_{\pi_2}(\tilde y)$

# 5.More than two-hypothesis test and model comparison. 

If you choose to be Bayesian you can relax in 2 directions.

* You can test more than two Hipothesis at no extra cost.
* You can test betwen any two models withot having to worry about one of them being nested into the other one.

$\underline{Example 1}$

Linear model $X_1, X_2$

$M_1 = y_1 | x_1, x_2 \sim N(\beta, \sigma^2) indep$
$M_2 = y_2 | x_1, x_2 \sim N(\beta_0 + \beta_{1}x_1, \sigma^2) indep$
$M_3 = y_3 | x_1, x_2 \sim N(\beta_0 + \beta_{2}x_2, \sigma^2) indep$
$M_4 = y_4 | x_1, x_2 \sim N(\beta_0 + \beta_{1}x_1 + \beta_{2}x_2, \sigma^2) indep$

A Bayesisan will start choosing:

$P(H_1) = \frac{1}{4}, \pi(\beta_0)$
$P(H_2) = \frac{1}{4}, \pi(\beta_0, \beta_1)$
$P(H_3) = \frac{1}{4}, \pi(\beta_0, \beta_2)$
$P(H_4) = \frac{1}{4}, \pi(\beta_0, \beta_1, \beta_2)$

and then compute the posterior

$P(H_1 | y) = \dots$
$P(H_2 | y) = \dots$
$P(H_3 | y) = \dots$
$P(H_4 | y) = \dots$

And then choose the one with greater posterior.

The diferent models are all nested.

$\underline{Example 2}$

Linear model $X_1, X_2$

$M_1 = y_1 | x_1, x_2 \sim N(\beta_0 + \beta_{1}x_1 + \beta_{2}x_2, \sigma^2) indep$
$M_2 = y_2 | x_1, x_2 \sim N(formula, \sigma^2) indep$

How do you choose?

Start assigning a a probability:

$P(H_1) = \frac{1}{2}, \pi(\beta_0, \beta_1, \beta_2)$
$P(H_2) = \frac{1}{2}, \pi(\theta_, \theta_2, \theta_3, \theta_4)$

And then compute the posterior:

$P(H_1 | y) = \dots$
$P(H_2 | y) = \dots$


If you have to choose betwen 

$M_1, M_2, \dots M_k$

$P(M_i | y) =  \frac{formula}{formula}$

(anotacins formula) Integrated likelihood.

(dibuix)

$formula per cada dibuix$

# 6.Prediction. 


# 7.Model averaging. 

$M_1 \beta_0 + \epsilon \leftrightarrow P_{\pi_1}(\tilde y|y) P(H_1 | y) = 0.001$
$M_2 \beta_0 + \beta_{1}x_1 + \epsilon \leftrightarrow P_{\pi_2}(\tilde y|y) P(H_2 | y) = 0.003$
$M_3 \beta_0 + \beta_{2}x_2 + \epsilon \leftrightarrow P_{\pi_3}(\tilde y|y) P(H_3 | y) = 0.37$
$M_4 \beta_0 + \beta_{1}x_1 + \beta_{2}x_2 + \epsilon \leftrightarrow P_{\pi_4}(\tilde y|y) P(H_4 | y) = 0.6296$

10 exploratori variables implies 1024 models.

What would you do? The winer takes all??

The thinks that cames natural is model averaging with the weight from the probability.



# 8.Simulation based inference. 
# 9.Frequentist asymptotic behavior of the posterior distribution. 
# 10.Bayesian asymptotic behavior of the posterior distribution. 
# 11.Decision theory and frequentist (Bayesian) assessment of the Bayesian (frequentist) inference. 
# 12.Summary.