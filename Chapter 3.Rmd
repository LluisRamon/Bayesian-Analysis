---
title: "Chapter 3 Hierachical Models"
author: "Lluis Ramon"
date: "1 de abril de 2014"
output: html_document
---

--- Start Class on 1-4-2014 (session 12)

# 1. Hierachical models (random effectsm multilevel models)

$\underline{Example}$

$y_i$ number of goals scored by Bar√ßa in one game in Spanish league.

$y = (y_1 \dots \y_n) \sim \prod_{i = 1}^{n} Poisson(\theta), \theta \in [0, \infty]$

Poission and Binomial are very rigid because when you model $\theta$ you are dealing with mean and variance.

It is belivable that the same value of $\theta^*$ applies to all observation in the sample?

It is more belivable to think

$y = (y_1 \dots \y_n) \sim \prod_{i = 1}^{n} Poisson(\theta_i), \theta = (\theta_1, \dots \theta_n) \in \Omega = [0, \infty]^n$

Does it make sense to start with this model? Does it make sense to model n y's through n $\theta_i$'s?

Modeling seems to make sense only if you summarize n observations $y_1 \dots \y_n$ through p parameters where p < n (Regression model).


$\underline{Example}$

$y_1$ number od votes for PP in the i-th area of Barcelona.

$Y = (y_1 \dots \y_n) \sim \prod_{i = 1}^{n} binomial(n_i, \theta), \theta \in [0,1]$

This assumes that the $\theta^*$ behind all $y_i$`s in the same, but that does not make sense.

$Y = (y_1 \dots \y_n) \sim \prod_{i = 1}^{n} binomial(n_i, \theta_i), \theta \in = (\theta_1, \dots \theta_n) \in \Omega = [0, 1]^n$

$\underline{Example}$

Assigment example with the dice.

You start with a huge box filled with dices. They are all with faces painted black or white in different proportions.

You do not know how where the dice painted.

Each time you roll a dice, it will be different but it will allways be comming from the box.

$Y = (y_1 \dots \y_n) \sim \prod_{i = 1}^{n} binomial(n, \theta_i), \theta_i \in {0, \frac{1}{6},  \frac{2}{6}, \frac{3}{6}, \frac{4}{6}, \frac{5}{6}, 1}$

$\underline{\theta} = (\theta_1, \dots, \theta_n) = \Omega_1 \times \Omega_2 \times \dots \times \Omega_n$

The main goal of gathering data is not learning about $\theta_i$ but learning about the distribution od $\theta_i$.

--- Start Class on 8-4-2014 (session 13)

$formula$

In log-linear Poisson models one assumes that

$formula$

This make sense because p << n
We want to do this without covariates:

$\underline{\theta} = (\theta_1, \dots, \theta_n) \in [0, \infty)^n$

You do this when you do not have the covariates nectked to explain your heterogeerities but you do not want to assume the heterogenezity does not exist.

We want to summarize n y's through n $\theta_i$'s.

Does this make sense?

Yes as long as you are willing to assume that all these $\theta_i$'s share the same unknown distribution.

The goal now will be to learn about this distribution of the $\theta_i$'s.

In general we will say that a $Y = (y_1 \dots \y_n)$ follows a Bayesian hierachical model if

$$Y = (y_1 \dots \y_n) | \underline{\theta} = (\theta_1, \dots, \theta_n) \sim \prod_{i = 1}^{n} P(y_i, \theta_i) \underline{\theta} = (\theta_1, \dots, \theta_n) \in \Omega \times \dots \times \Omega$$

$\underline{\theta} = (\theta_1, \dots, \theta_n) \sim \prod_{i = 1}^{n} \pi(\theta_i, \gamma)$

$\gamma$ new parameter (unknown value) hyperparameter.

$\gamma \sim \psi (\gamma)$ prior distribution on $\gamma$ (known).

This is not anything new. This is a special case of a Bayesian model. 

Formulation a)

$\underline{y} | \underline{\theta} \sim formula$
Statistical model

$formula$
prior

Formulation b)

$formula$
Statistical model

$formula$
prior

If you care about the $\underline{\theta}$ you would choose a).
If you care about $\gamma$ 

# 2.Empirical Bayes.

